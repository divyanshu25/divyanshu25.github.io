<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Home</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Home</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image Query</title>
      <link>/project/imagequery/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/project/imagequery/</guid>
      <description>&lt;h1 id=&#34;abstracta-idabstracta&#34;&gt;Abstract&lt;a id=&#34;abstract&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Querying images has traditionally been implemented using metadata of an image, where images are not treated as first class citizens in a database. This approach depends solely on how well an image has been tagged by a human user. In this project we aim to change the status quo by also including the images itself in the search process. At its core, our project focuses on looking at an image to extract relevant features, which in turn can be used to generate textual descriptions of the image. We then compute various similarity measures between these textual descriptions and a user provided query to return the most relevant images. We then extend this system to search similar images given an input image.&lt;/p&gt;
&lt;h1 id=&#34;introductiona-idintroductiona&#34;&gt;Introduction&lt;a id=&#34;introduction&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Image search using textual description is a very powerful tool with numerous applications. In contrast to the traditional setting, where image retrieval is performed by looking up metadata, we propose to build a system that can query an image by using its features to generate a textual description. Writing metadata for a ever-growing set of images is an intractable task. In most cases the images either don&amp;rsquo;t have metadata or have incorrect metadata. This project aims to break that dependency and use the implicit information within the image for retrieval.&lt;/p&gt;
&lt;p&gt;Our model is built on an image captioning system and computing various similarity measures to fetch images based on the closeness between textual query and generated image captions. In this project we have used two architectures, one is called vanilla architecture where we use a CNN-encoder to generate image features and a LSTM model as decoder, and second is attention architecture where we again use a CNN-encoder to generate image features and an attention based LSTM model as decoder. We will go into details of both these architectures in subsequent sections. We are using COCO and Flickr8k datasets, both these datasets provide 5 human generated captions per image. The main modules that are implemented in this project are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Implemented both vanilla and attention architectures, and tested both the architectures on various hyper-parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implemented a module to compute BLEU score of both vanilla and attention architecture on COCO and Flickr8k datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have developed an weighted cosine-similarity mechanism by computing the cosine-similarity between embeddings of textual query and generated captions and weighting them with inverse word frequency to fetch top 5 relevant images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implemented an REST-API architecture to query images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extended the image search system to search based on images instead of the textual description.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;related-worka-idrelated-worka&#34;&gt;Related Work&lt;a id=&#34;related-work&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Deep Fragment Embeddings for Bidirectional Image Sentence Mapping&lt;/strong&gt;: Our initial inspiration for image retrieval from came from 
&lt;a href=&#34;http://arxiv.org/abs/1406.5679&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep fragment embeddings&lt;/a&gt;. Here the authors use a object detection model to extract fragments of images (objects). They also extract fragments of sentences. Fragments from both images and sentences are embedded in a common embedding space and explicitly reasons about their latent, inter-modal correspondences. They use a cosine-similarity based loss function where they take embeddings from image space and sentence space compute there dot product.&lt;/p&gt;
&lt;p&gt;Our project is based on a similar approach but instead of comparing common embeddings from images and sentences we use a image captioning system to generate novel captions for an images and then use those captions to compute similarity between the given query. Our loss function (weighted-cosine similarity) is similar to their loss function, but differs in the fact that they compute fragment level loss whereas we compute a global loss.&lt;/p&gt;
&lt;h1 id=&#34;methodologya-idmethodologya&#34;&gt;Methodology&lt;a id=&#34;methodology&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;At core of our project we use an image captioning model to generate captions for an image, then we store those captions in a database. Then on being provided with a textual query we iterate over all the generated captions in the database, compute the various similarity metrics between the user textual query and generated captions and finally return the images with top5 similarity scores. We have used two datasets - Flickr8k and COCO , to evaluate our model. Both these datasets provide 5 user generated captions per image. In subsequent section we will go over various architecture that we experimented with in our captioning model.&lt;/p&gt;
&lt;h2 id=&#34;image-captioning-modela-idimage-captioning-modela&#34;&gt;Image Captioning Model&lt;a id=&#34;image-captioning-model&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;vanilla-cnn-lstm-networka-idvanilla-cnn-lstm-networka&#34;&gt;Vanilla CNN-LSTM network&lt;a id=&#34;vanilla-cnn-lstm-network&#34;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The vanilla architecture uses a pretrained Resnet50 model to encode the image features. These features are concatenated with the embeddings of true captions and fed to a single layered Long Short Term Memory (LSTM) network. The embedding and LSTM layers are trained from scratch. The outputs of the trained LSTM are used to generate the predicted captions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Images/vanilla_model_arch.png&#34; alt=&#34;img&#34; title=&#34;Vanilla Model Architecture&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;cnn-lstm-with-attentiona-idcnn-lstm-with-attentiona&#34;&gt;CNN-LSTM with attention&lt;a id=&#34;cnn-lstm-with-attention&#34;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In this architecture we use an attention mechanism in a single layered LSTM to give attention to image features along with previous generated word. The approach is taken from the paper &amp;ldquo;Show, attend and tell&amp;rdquo; . We use a pretrained Resnet101 model to generate image features. At each step of LSTM the image features and the last hidden state of LSTM is passed through an attention layer. The output of the attention layer is concatenated with the embeddings of the word generated in last step of LSTM to generate the current word.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Images/Attention_model_arch.jpeg&#34; alt=&#34;img&#34; title=&#34;Attention Model Architecture&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;cnn-lstm-with-attention-using-bert-embeddinga-idcnn-lstm-with-attention-using-bert-embeddinga&#34;&gt;CNN-LSTM with attention using BERT embedding&lt;a id=&#34;cnn-lstm-with-attention-using-bert-embedding&#34;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In this variant, we used our attention CNN-LSTM network along with a pretrained BERT embedding layer instead of training the embedding layer from scratch. The pretrained BERT model already has learnt language embeddings over a large dataset and has proven to be successful in many downstream language tasks. We load the pretrained weights in the embedding layer and then fine-tune it for our own down stream task.&lt;/p&gt;
&lt;h2 id=&#34;caption-generation-and-search-apia-idcaption-generation-and-search-apia&#34;&gt;Caption Generation and Search API&lt;a id=&#34;caption-generation-and-search-api&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;textual-searcha-idtextual-searcha&#34;&gt;Textual search&lt;a id=&#34;textual-search&#34;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We use beam search to sample top five predictions for each image in our test set and saved these to an Sqlite database. Now when a textual query is provided we iterate over all the images in the database and compute the similarity between the embeddings of textual query and each of the 5 generated captions for a particular image and store the max value. After iterating over all the images we return the images with top 5 similarity scores. To compute the similarity we experiment with two metrics, BLEU-score and weighted cosine similarity between embedded captions and query. Details of both these similarity metrics will be covered in a later section. To generate the embeddings we have experimented with two variants:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We generate embeddings using the embedding layer that was trained from scratch during the training process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We use the pre-trained BERT model&amp;rsquo;s embedding layer and further train it and use it to generate the embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;similar-image-searcha-idsimilar-image-searcha&#34;&gt;Similar image search&lt;a id=&#34;similar-image-search&#34;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We then extended the above textual search to search for similar images, given an input image. To do this we first generate the caption of the input image and then use the above search mechanism to produce the related images. The quality of the produced results directly depends on the image captioning system and this corroborates with the outputs we observed in our experiments.&lt;/p&gt;
&lt;h1 id=&#34;experimentsa-idexperimentsa&#34;&gt;Experiments&lt;a id=&#34;experiments&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;We experiment with two datasets: Flickr8k and COCO.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flickr8k&lt;/strong&gt; contains 8,091 images collected from Flickr. The dataset in composed of images depicting people and animals doing various activities. Each image consist of 5 human generated captions. We use publicly available splits for train, validation and test sets. Validation and train sets consists of 1000 images each.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;COCO&lt;/strong&gt; is the largest image captioning dataset, containing 118,000, 5,000 and 40,670 images for training, validation and test respectively. This dataset is more challenging, since most images contain multiple objects in the context of complex scenes. Each image has 5 human annotated captions. To ease our training and evaluation times, we use 30000 images from the training set for training, 1000 images from the validation set for validation and 1500 images from the withheld validation set for testing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-processing&lt;/strong&gt; We truncate captions longer than 40 words for COCO and for Flickr8k. We then build a vocabulary of words that occur at least 5 times in the training set, resulting in 11372 and 3005 words for COCO and Flickr8k respectively. When using pretrained bert-embeddings, we use bert tokenizer which has a vocabulary size of 30,522.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hyper-Paramters:&lt;/strong&gt; We use the following hyper-parameters to train our model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: 0.001, &lt;strong&gt;Batch Size&lt;/strong&gt;: 128, &lt;strong&gt;Optimizer&lt;/strong&gt;: ADAM, &lt;strong&gt;Momentum&lt;/strong&gt;: 0.9, &lt;strong&gt;Weight Decay&lt;/strong&gt;: 0.99, &lt;strong&gt;Scheduler Weight Decay&lt;/strong&gt;: 0.95, &lt;strong&gt;Dropout&lt;/strong&gt;: 0.5&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Size&lt;/strong&gt;: 512, &lt;strong&gt;Hidden Size&lt;/strong&gt;: 512, &lt;strong&gt;Image Feature Size&lt;/strong&gt;: 2048, &lt;strong&gt;Attention vector Size&lt;/strong&gt;: 512, &lt;strong&gt;Encoded Image Size&lt;/strong&gt;: 14&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Compared Approaches:&lt;/strong&gt; For offline evaluation on Flickr8k and COCO , we compare the following models in this project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vanilla Model (Vanilla CNN-LSTM), In this model we use features generated by Resnet50 CNN model and pass it to a LSTM model to predict output words.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Attention model (CNN-LSTM with Attention) which uses a hard attention mechanism to predict words in LSTM decoder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Attention model with BERT embeddings. In this model we train our CNN-LSTM with Attention model with pre-trained BERT embeddings and also fine tune the embedding layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal of this comparison is to verify that the improvements are caused specifically by the attention mechanism and BERT embeddings, and are not the result of orthogonal contributions (e.g. better CNN features or better optimization). We further compare our method with Adaptive Attention , Google NIC and Hard-Attention .&lt;/p&gt;
&lt;h1 id=&#34;resultsa-idresultsa&#34;&gt;Results&lt;a id=&#34;results&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id=&#34;training-mechanisma-idtraining-mechanisma&#34;&gt;Training mechanism:&lt;a id=&#34;training-mechanism&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Using the above hyperparameters, we trained all three models on the two datasets for 20 epochs each. The graphs in Fig [1,2,3,4] show the training and validation loss for each of these.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Images/flickr_train_loss.png&#34; alt=&#34;img&#34; title=&#34;Training loss on Flickr8k dataset.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Images/flickr_val_loss.png&#34; alt=&#34;img&#34; title=&#34;Validation loss on Flickr8k dataset.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Images/Coco_train_loss.png&#34; alt=&#34;img&#34; title=&#34;Training loss on COCO dataset.&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Images/Coco_val_loss.png&#34; alt=&#34;img&#34; title=&#34;Validation loss on COCO dataset.&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;quantitative-analysisa-idquantitative-analysisa&#34;&gt;Quantitative Analysis:&lt;a id=&#34;quantitative-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We report our results for evaluating the image captioning model on COCO and Flickr8k datasets using BLEU score. 
&lt;a href=&#34;#bleu_score_table&#34;&gt;Table&lt;/a&gt; shows the BLEU score with 1,2,3 and 4-grams for both our vanilla and attention models trained over COCO and Flickr8k datasets.&lt;/p&gt;
&lt;p&gt;In search API to fetch top-5 relevant images for a given query we use the following metrics.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BLEU-Score&lt;/strong&gt;: We used BLEU-score as a similarity metric, where we take top 5 generated captions from an image and compute the BLEU score against the user given query. We return the results with top-5 BLEU scores. In this approach we also experimented with BLEU-1, BLEU-2, BLEU-3, BLEU-4, n-grams and found that BLEU-1 and BLEU-2 give the best results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Weighted Cosine Similarity:&lt;/strong&gt; In this approach we compute the embeddings the user query and caption generated by our model using the learned embedding layer. The embeddings are a 2D tensor of shape (caption length × embed dimension). Now for each caption in our database, we compute the matrix multiplication of the query&amp;rsquo;s embedding with the caption&amp;rsquo;s embedding. For example, if the query length is 3, the caption length is 4 and the embedding length is 768, the query&amp;rsquo;s embedding (Q) would be of size (3×768) and the caption&amp;rsquo;s embedding (C) would be of size (4×768). We now compute (Q×C&amp;rsquo;) to get a (3×4) dimension matrix. We then take a max along the rows of the matrix to get (3×1) vector which represents the similarity of the query and the caption. We then weight the obtained vector by inverse word-frequency and then finally we sum it up to obtain a similarity score. The figure below illustrates the process. In the figure &amp;ldquo;A dog is sleeping&amp;rdquo; is the generated caption and &amp;ldquo;Dog is running&amp;rdquo; is user given query.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Images/weighted_sim_score.png&#34; alt=&#34;img&#34; title=&#34;Weighted Similarity Score.&#34;&gt;
&lt;a id=&#34;bleu_score_table&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;BLEU Scores&lt;/th&gt;
&lt;th&gt;Flickr8k&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;MS-COCO&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Architecture&lt;/td&gt;
&lt;td&gt;BLEU-1&lt;/td&gt;
&lt;td&gt;BLEU-2&lt;/td&gt;
&lt;td&gt;BLEU-3&lt;/td&gt;
&lt;td&gt;BLEU-4&lt;/td&gt;
&lt;td&gt;BLEU-1&lt;/td&gt;
&lt;td&gt;BLEU-2&lt;/td&gt;
&lt;td&gt;BLEU-3&lt;/td&gt;
&lt;td&gt;BLEU-4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vanilla&lt;/td&gt;
&lt;td&gt;0.506&lt;/td&gt;
&lt;td&gt;0.304&lt;/td&gt;
&lt;td&gt;0.191&lt;/td&gt;
&lt;td&gt;0.121&lt;/td&gt;
&lt;td&gt;0.622&lt;/td&gt;
&lt;td&gt;0.412&lt;/td&gt;
&lt;td&gt;0.281&lt;/td&gt;
&lt;td&gt;0.200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Attention&lt;/td&gt;
&lt;td&gt;0.597&lt;/td&gt;
&lt;td&gt;0.405&lt;/td&gt;
&lt;td&gt;0.278&lt;/td&gt;
&lt;td&gt;0.189&lt;/td&gt;
&lt;td&gt;0.671&lt;/td&gt;
&lt;td&gt;0.472&lt;/td&gt;
&lt;td&gt;0.338&lt;/td&gt;
&lt;td&gt;0.245&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Attention with BERT&lt;/td&gt;
&lt;td&gt;0.605&lt;/td&gt;
&lt;td&gt;0.415&lt;/td&gt;
&lt;td&gt;0.286&lt;/td&gt;
&lt;td&gt;0.196&lt;/td&gt;
&lt;td&gt;0.614&lt;/td&gt;
&lt;td&gt;0.421&lt;/td&gt;
&lt;td&gt;0.294&lt;/td&gt;
&lt;td&gt;0.210&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hard-Attention&lt;/td&gt;
&lt;td&gt;0.670&lt;/td&gt;
&lt;td&gt;0.457&lt;/td&gt;
&lt;td&gt;0.314&lt;/td&gt;
&lt;td&gt;0.213&lt;/td&gt;
&lt;td&gt;0.718&lt;/td&gt;
&lt;td&gt;0.504&lt;/td&gt;
&lt;td&gt;0.357&lt;/td&gt;
&lt;td&gt;0.250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adaptive Attention&lt;/td&gt;
&lt;td&gt;0.677&lt;/td&gt;
&lt;td&gt;0.494&lt;/td&gt;
&lt;td&gt;0.354&lt;/td&gt;
&lt;td&gt;0.251&lt;/td&gt;
&lt;td&gt;0.742&lt;/td&gt;
&lt;td&gt;0.580&lt;/td&gt;
&lt;td&gt;0.439&lt;/td&gt;
&lt;td&gt;0.332&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Google NIC&lt;/td&gt;
&lt;td&gt;0.630&lt;/td&gt;
&lt;td&gt;0.410&lt;/td&gt;
&lt;td&gt;0.270&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;0.666&lt;/td&gt;
&lt;td&gt;0.461&lt;/td&gt;
&lt;td&gt;0.329&lt;/td&gt;
&lt;td&gt;0.246&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;qualitative-analysisa-idqualitative-analysisa&#34;&gt;Qualitative Analysis&lt;a id=&#34;qualitative-analysis&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Based on qualitative analysis of our search results, we tried multiple similarity metrics to find matching images for the search query from our database. In 
&lt;a href=&#34;#subsec:searchres&#34;&gt;Appendix&lt;/a&gt; show examples of query results searched on both datasets using captions generated from all three model variants.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We first generated search results based on the highest BLEU-1 and BLEU-2 score calculated between the query and the predicted captions for each image. Since we are storing five predicted captions per image, the summation of BLEU scores with all predicted captions of the image was taken to be the similarity score of the image with the query. The major drawback in this method was the equal weightage being given to &amp;ldquo;filler&amp;rdquo; words in the query like prepositions and conjunctions. This sometimes inflated the score of long but irrelevant captions in the database and led to many irrelevant results being thrown out along with relevant ones.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To avoid above drawback, we filtered all &amp;ldquo;stopwords&amp;rdquo; from the query and captions before computing BLEU score. This removed filler words from the comparison and significantly improved search results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On further inspection, we observed that for good results we needed to give more importance to the rarely occurring words in our corpus as compared to more commonly occurring words. For example, in a query like &amp;ldquo;A person sitting with a laptop&amp;rdquo;, &amp;ldquo;person&amp;rdquo; is a very common word present in many captions but &amp;ldquo;laptop&amp;rdquo; is a rarer word. Our current metrics give equal importance to the two words, throwing results with just a person in them if it could not find both person and laptop. However, we consider results containing a laptop to be more relevant in this case. Hence, we also multiplied &amp;ldquo;term weightages&amp;rdquo; with count of common unigrams while calculating BLEU scores. Term weightages were calculated from the term counts (TC) of each word in the test corpus as follows: &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=w_i = 1 - \frac{TC_i}{\sum_i{TC_i}}&#34;&gt; This gave us significantly better search results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One drawback still remaining in these methods is that we are looking for common words between query and captions, but any words with similar meaning that could also give relevant results are being ignored. In order to capture this similarity, we used the inner product of the BERT embeddings of the two sentences, as the similarity score instead of BLEU score. After applying term weightage to this method as well, we observe good search results.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In section 
&lt;a href=&#34;#subsec:searchres&#34;&gt;Search Results&lt;/a&gt;, we show the results for &amp;ldquo;Search by image&amp;rdquo; API. The input image is randomly picked from google. We see an image with dolphins being returned in the results as the model has predicted its caption as &amp;ldquo;A man surfing on a wave&amp;rdquo;.&lt;/p&gt;
&lt;h1 id=&#34;conclusiona-idconclusiona&#34;&gt;Conclusion&lt;a id=&#34;conclusion&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;In this project we started out with the aim of retrieving images based on textual queries whilst treating images as first class citizens rather than relying on meta-data of an image. We experimented with various image captioning models namely Vanilla CNN-LSTM network, CNN-LSTM with attention, CNN-LSTM using BERT embeddings. We evaluated the quality of captions generated by each of the model using BLEU score which we reported in table 
&lt;a href=&#34;#bleu_score_table&#34;&gt;Blue Scores&lt;/a&gt;. Scores obtained by our models were fairly close to the referred models. We then implemented a Search API which given a user input query returned top 5 relevant images. We used two metrics at the core of our search API, BLEU-Score and cosine similarity. We used these scores with and without term weights and discovered that results obtained by weighted scores were much better than normal BLEU-score or cosine similarity. Within various n-grams of BLEU score, BLEU-1 and BLEU-2 gave the best results. As an addition we extended the textual search to incorporate image based search. We observed that the quality of the image based search depends on the quality of the captions returned by the captiong model.&lt;/p&gt;
&lt;h1 id=&#34;appendicesa-idappendicesa&#34;&gt;Appendices&lt;a id=&#34;appendices&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id=&#34;search-resultsa-idsubsecsearchresa&#34;&gt;Search results&lt;a id=&#34;subsec:searchres&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;coco-vanilla-results&#34;&gt;Coco Vanilla Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Images/coco_vanilla.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;coco-attention-results&#34;&gt;Coco Attention Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Images/coco_attn.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;coco-attention-with-bert-results&#34;&gt;Coco Attention with BERT Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Images/coco_attn_bert.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;flickr-vanilla-results&#34;&gt;Flickr Vanilla Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Images/flickr_vanilla.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;flickr-attention-results&#34;&gt;Flickr Attention Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Images/flickr_attn.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;flickr-attention-with-bert-results&#34;&gt;Flickr Attention with BERT Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;Images/flickr_attn_bert.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;computing-infrastructurea-idcomputing-infrastructurea&#34;&gt;Computing Infrastructure&lt;a id=&#34;computing-infrastructure&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The computing infrastructure used for training and running the models described in the paper was 1 NVIDIA Tesla K80 GPU.&lt;/p&gt;
&lt;h2 id=&#34;runtimea-idruntimea&#34;&gt;Runtime&lt;a id=&#34;runtime&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The average training time for the model on each combination of hyperparameters was roughly 7 hours for 20 epochs.&lt;/p&gt;
&lt;h2 id=&#34;source-codea-idsource-codea&#34;&gt;Source Code&lt;a id=&#34;source-code&#34;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The code is publicly available at 
&lt;a href=&#34;https://github.com/divyanshu25/ImageQuery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repo&lt;/a&gt;: &lt;a href=&#34;https://github.com/divyanshu25/ImageQuery&#34;&gt;https://github.com/divyanshu25/ImageQuery&lt;/a&gt;. The readme contains details on how to reproduce the results.&lt;/p&gt;
&lt;h3 id=&#34;referred-codebasesa-idreferred-codebasesa&#34;&gt;Referred Codebases&lt;a id=&#34;referred-codebases&#34;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Noob-can-Compile/Automatic-Image-Captioning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Image Captioning&lt;/a&gt;: &lt;a href=&#34;https://github.com/Noob-can-Compile/Automatic-Image-Captioning/&#34;&gt;https://github.com/Noob-can-Compile/Automatic-Image-Captioning/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Image Captioning with Attention&lt;/a&gt;: &lt;a href=&#34;https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning&#34;&gt;https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deploying Flask Application using Apache with mod_wsgi</title>
      <link>/post/flask-apache/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/flask-apache/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;We have written this blog to help developers deploy &lt;code&gt;flask&lt;/code&gt; application with &lt;code&gt;apache&lt;/code&gt; and &lt;code&gt;mod_wsgi&lt;/code&gt; in production.
We read through a lot of articles and found out the steps we layout below are the simplest way to deploy.
Also, the biggest caveat to deploying in production is version mismatches, so we are going to be building things from source rather than using apt-get to install packages.
So let&amp;rsquo;s get started.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;orgbba5361&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;build-and-install-custom-version-of-python&#34;&gt;Build and install custom version of python&lt;/h1&gt;
&lt;p&gt;We used python &lt;code&gt;3.7.5&lt;/code&gt; for a current project and here is how you can deploy python from source. Make sure you remove any other python version you might have on your machine.
To remove any existing python installations.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get purge python&amp;lt;version&amp;gt;
sudo apt-get autoremove
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- Refer link for compilation: [Link](https://linuxize.com/post/how-to-install-python-3-7-on-ubuntu-18-04/)
--&gt;
&lt;p&gt;&lt;a id=&#34;orgb970147&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;1-install-dependencies-and-packages&#34;&gt;1. Install Dependencies and packages.&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;sudo apt update
sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev libffi-dev wget libbz2-dev
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-download-source&#34;&gt;2. Download source&lt;/h3&gt;
&lt;p&gt;Now to fetch tar file for a specific python version use.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can check out other python versions here: 
&lt;a href=&#34;https://www.python.org/downloads/source/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;orge79fbcd&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;3-compile-python&#34;&gt;3. Compile python&lt;/h3&gt;
&lt;p&gt;Now to compile the downloaded tar.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tar -xf Python-3.7.5.tgz
cd Python-3.7.5
./configure --enable-shared --prefix=/usr/ 
make -j4
sudo make install 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Argument in make -j&lt;code&gt;N&lt;/code&gt; is the number of processors if you are not sure about number of processors in the machine. You can use
&lt;code&gt;nproc&lt;/code&gt; to get it.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;org895c7b7&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;build-and-install-custom-version-of-wsgi-mod&#34;&gt;Build and install custom version of wsgi mod&lt;/h1&gt;
&lt;p&gt;Ok, now we move on to getting mod_wsgi installation right. Before we get into steps I would like you to read this

&lt;a href=&#34;https://modwsgi.readthedocs.io/en/develop/user-guides/virtual-environments.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;.
which essentially says this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The problem in trying to force mod_wsgi to use a different Python installation than what it was compiled for, 
even where it is the same Python version, is that the Python installation may itself not have been 
compiled with the same options. This is especially a problem when it comes to issues around how Python stores 
Unicode characters in memory. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So to avoid this problem we are going to build mod_wsgi from the source and compile it with python we just installed in the last step.&lt;/p&gt;
&lt;h3 id=&#34;1-compile-wsgi-mod&#34;&gt;1. Compile wsgi-mod&lt;/h3&gt;
&lt;p&gt;Download source code from: 
&lt;a href=&#34;https://github.com/GrahamDumpleton/mod_wsgi/releases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github releases&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tar xvfz mod_wsgi-X.Y.tar.gz
cd mod_wsgi-X.Y
sudo apt-get install apache2-dev
./configure --with-python=/usr/bin/python
make -j4
sudo make install
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- Refer link for compilation: [wsgi](https://modwsgi.readthedocs.io/en/develop/user-guides/quick-installation-guide.html)
--&gt;
&lt;h1 id=&#34;configure-apache&#34;&gt;Configure Apache&lt;/h1&gt;
&lt;p&gt;Ok, now we move on to building apache2.&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;orgd89c968&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;1-install-apache2&#34;&gt;1. Install apache2&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install apache2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a id=&#34;orga12ca17&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-configure-wsgi-module&#34;&gt;2. Configure Wsgi module&lt;/h3&gt;
&lt;p&gt;Now we need to configure apache with the mod_wsgi we complied and installed in the last step.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Create a file: `/etc/apache2/mods-available/wsgi.load`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the following line to the file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;`LoadModule wsgi_module /usr/lib/apache2/modules/mod_wsgi.so`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to enable mod_wsgi and restart apache2.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo a2enmod wsgi
sudo systemctl restart apache2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This completes the installation of Apache2 and mod_wsgi, all complied with python&lt;code&gt;3.7.5&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;deploying-flask-application&#34;&gt;Deploying Flask Application&lt;/h1&gt;
&lt;h3 id=&#34;1-create-wsgi-file&#34;&gt;1. Create .wsgi file&lt;/h3&gt;
&lt;p&gt;Now to deploy the flask application copy your code directory to
&lt;code&gt;/var/www/&lt;/code&gt; directory and create &lt;code&gt;&amp;lt;your_application&amp;gt;&lt;/code&gt;.wsgi file and add the following lines.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import sys
sys.path.insert(0, &#39;/var/www/&amp;lt;your_application_dir&amp;gt;&#39;)
from &amp;lt;your_application&amp;gt; import &amp;lt;flask_app&amp;gt; as application
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-create-virtualenv&#34;&gt;2. Create virtualenv&lt;/h3&gt;
&lt;p&gt;Its generally best practice to create a virtualenv for your application and install all the dependencies, to avoid clashing package versions that
other application might be using. To create a virtualenv:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /var/www/
virtualenv -p python3 &amp;lt;venv_name&amp;gt;
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that you will need a &lt;code&gt;requirments.txt&lt;/code&gt; file. To create one you can do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip freeze &amp;gt; requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3-create-configuration-file-in-apache&#34;&gt;3. Create Configuration file in Apache&lt;/h3&gt;
&lt;p&gt;Now we need to create a configuration file to tell apache from where to load your application.
In &lt;code&gt;/etc/apache2/sites-available&lt;/code&gt; create &lt;code&gt;&amp;lt;your_application&amp;gt;.conf&lt;/code&gt; file and add the following lines.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;    &amp;lt;VirtualHost *:80&amp;gt;
       ServerName &amp;lt;Server_name&amp;gt;
       WSGIDaemonProcess &amp;lt;your_application&amp;gt; user=&amp;lt;user&amp;gt; group=&amp;lt;group&amp;gt; threads=5 \
       python-home=/var/www/&amp;lt;venv_name&amp;gt;
   
       WSGIProcessGroup &amp;lt;your_application&amp;gt;
       WSGIApplicationGroup %{GLOBAL}
       WSGIScriptAlias / /var/www/&amp;lt;your_application_dir&amp;gt;/&amp;lt;your_application&amp;gt;.wsgi
       &amp;lt;Directory /var/www/&amp;lt;your_application_dir&amp;gt;/&amp;lt;your_application&amp;gt;/&amp;gt;
           Order allow,deny
           Allow from all
           Require all granted
       &amp;lt;/Directory&amp;gt;
       Alias /static /var/www/&amp;lt;your_application_dir&amp;gt;/&amp;lt;your_application&amp;gt;/static
       &amp;lt;Directory /var/www/&amp;lt;your_application_dir&amp;gt;/&amp;lt;your_application&amp;gt;/static/&amp;gt;
           Order allow,deny
           Allow from all
        &amp;lt;/Directory&amp;gt;
        ErrorLog /var/www/&amp;lt;your_application_dir&amp;gt;/error.log
        LogLevel debug
        CustomLog /var/www/&amp;lt;your_application_dir&amp;gt;/access.log combined
   &amp;lt;/VirtualHost&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can remove that logging commands if you don&amp;rsquo;t wish to see logs.&lt;/p&gt;
&lt;h3 id=&#34;4-enable-application&#34;&gt;4. Enable application&lt;/h3&gt;
&lt;p&gt;Now we need to tell apache to enable our application and load the configuration file we just created.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo a2ensite &amp;lt;your_application&amp;gt;
sudo systemctl restart apache2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s it. Your website is up and running on the public IP for the server.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Extracting Rock Outcrop from Antarctic Landsat Imagery using Semantic Segmentation</title>
      <link>/project/rockoutcrop/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/project/rockoutcrop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gray Scale Video Colorization</title>
      <link>/project/gray-video/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/project/gray-video/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Engineering High-Throughput, Low-Latency Machine Learning Services.</title>
      <link>/post/urp/</link>
      <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
      <guid>/post/urp/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;#title: &amp;lsquo;Engineering High-Throughput, Low-Latency Machine Learning Services.&amp;rsquo;
#subtitle: &amp;lsquo;Serve your Machine Learning Models in Real-Time.&amp;rsquo;
#&lt;!-- summary: Create a beautifully simple website in under 10 minutes. --&gt;
#authors:
#- admin
#- Shirsh Bansal
#tags:
#- Distributed Computing
#- Machine Learning Systems#&lt;/p&gt;
&lt;p&gt;#date: &amp;ldquo;2016-04-20T00:00:00Z&amp;rdquo;
#lastmod: &amp;ldquo;2019-04-17T00:00:00Z&amp;rdquo;
#featured: false
#draft: false#&lt;/p&gt;
&lt;h2 id=&#34;featured-image&#34;&gt;Featured image&lt;/h2&gt;
&lt;h2 id=&#34;to-use-add-an-image-named-featuredjpgpng-to-your-pages-folder&#34;&gt;To use, add an image named &lt;code&gt;featured.jpg/png&lt;/code&gt; to your page&amp;rsquo;s folder.&lt;/h2&gt;
&lt;h2 id=&#34;placement-options-1--full-column-width-2--out-set-3--screen-width&#34;&gt;Placement options: 1 = Full column width, 2 = Out-set, 3 = Screen-width&lt;/h2&gt;
&lt;h2 id=&#34;focal-point-options-smart-center-topleft-top-topright-left-right-bottomleft-bottom-bottomright&#34;&gt;Focal point options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight&lt;/h2&gt;
&lt;p&gt;#image:&lt;/p&gt;
&lt;h1 id=&#34;placement-2&#34;&gt;placement: 2&lt;/h1&gt;
&lt;h1 id=&#34;caption-image-credit-unsplashhttpsunsplashcomphotoscpkojocxduy&#34;&gt;caption: &amp;lsquo;Image credit: 
&lt;a href=&#34;https://unsplash.com/photos/CpkOjOcXdUY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Unsplash&lt;/strong&gt;&lt;/a&gt;&amp;rsquo;&lt;/h1&gt;
&lt;h1 id=&#34;focal_point-&#34;&gt;focal_point: &amp;quot;&amp;rdquo;&lt;/h1&gt;
&lt;h1 id=&#34;preview_only-false&#34;&gt;preview_only: false#&lt;/h1&gt;
&lt;h2 id=&#34;projects-optional&#34;&gt;Projects (optional).&lt;/h2&gt;
&lt;h2 id=&#34;associate-this-post-with-one-or-more-of-your-projects&#34;&gt;Associate this post with one or more of your projects.&lt;/h2&gt;
&lt;h2 id=&#34;simply-enter-your-projects-folder-or-file-name-without-extension&#34;&gt;Simply enter your project&amp;rsquo;s folder or file name without extension.&lt;/h2&gt;
&lt;h2 id=&#34;eg-projects--internal-project-references-contentprojectdeep-learningindexmd&#34;&gt;E.g. &lt;code&gt;projects = [&amp;quot;internal-project&amp;quot;]&lt;/code&gt; references &lt;code&gt;content/project/deep-learning/index.md&lt;/code&gt;.&lt;/h2&gt;
&lt;h2 id=&#34;otherwise-set-projects--&#34;&gt;Otherwise, set &lt;code&gt;projects = []&lt;/code&gt;.&lt;/h2&gt;
&lt;p&gt;#projects: []
#&amp;mdash;
#external_link: &lt;a href=&#34;https://medium.com/adobetech/engineering-high-throughput-low-latency-machine-learning-services-7d45edac0271&#34;&gt;https://medium.com/adobetech/engineering-high-throughput-low-latency-machine-learning-services-7d45edac0271&lt;/a&gt;&lt;/p&gt;
&lt;!-- The following blog has been moved to Medium under **Adobe Tech Blogs**. 

Please visit the following [link.](https://medium.com/adobetech/engineering-high-throughput-low-latency-machine-learning-services-7d45edac0271) --&gt;
&lt;!-- **Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 _widgets_, _themes_, and _language packs_ included!**

[Check out the latest **demo**](https://academic-demo.netlify.com/) of what you&#39;ll get in less than 10 minutes, or [view the **showcase**](https://sourcethemes.com/academic/#expo) of personal, project, and business sites.

- 👉 [**Get Started**](#install)
- 📚 [View the **documentation**](https://sourcethemes.com/academic/docs/)
- 💬 [**Ask a question** on the forum](https://discourse.gohugo.io)
- 👥 [Chat with the **community**](https://spectrum.chat/academic)
- 🐦 Twitter: [@source_themes](https://twitter.com/source_themes) [@GeorgeCushen](https://twitter.com/GeorgeCushen) [#MadeWithAcademic](https://twitter.com/search?q=%23MadeWithAcademic&amp;src=typd)
- 💡 [Request a **feature** or report a **bug**](https://github.com/gcushen/hugo-academic/issues)
- ⬆️ **Updating?** View the [Update Guide](https://sourcethemes.com/academic/docs/update/) and [Release Notes](https://sourcethemes.com/academic/updates/)
- ❤️ **Support development** of Academic:
  - ☕️ [**Donate a coffee**](https://paypal.me/cushen)
  - 💵 [Become a backer on **Patreon**](https://www.patreon.com/cushen)
  - 🖼️ [Decorate your laptop or journal with an Academic **sticker**](https://www.redbubble.com/people/neutreno/works/34387919-academic)
  - 👕 [Wear the **T-shirt**](https://academic.threadless.com/)
  - 👩‍💻 [**Contribute**](https://sourcethemes.com/academic/docs/contribute/)

















&lt;figure id=&#34;figure-academic-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; data-caption=&#34;Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34;&gt;


  &lt;img src=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.
  &lt;/figcaption&gt;


&lt;/figure&gt;


**Key features:**

- **Page builder** - Create *anything* with [**widgets**](https://sourcethemes.com/academic/docs/page-builder/) and [**elements**](https://sourcethemes.com/academic/docs/writing-markdown-latex/)
- **Edit any type of content** - Blog posts, publications, talks, slides, projects, and more!
- **Create content** in [**Markdown**](https://sourcethemes.com/academic/docs/writing-markdown-latex/), [**Jupyter**](https://sourcethemes.com/academic/docs/jupyter/), or [**RStudio**](https://sourcethemes.com/academic/docs/install/#install-with-rstudio)
- **Plugin System** - Fully customizable [**color** and **font themes**](https://sourcethemes.com/academic/themes/)
- **Display Code and Math** - Code highlighting and [LaTeX math](https://en.wikibooks.org/wiki/LaTeX/Mathematics) supported
- **Integrations** - [Google Analytics](https://analytics.google.com), [Disqus commenting](https://disqus.com), Maps, Contact Forms, and more!
- **Beautiful Site** - Simple and refreshing one page design
- **Industry-Leading SEO** - Help get your website found on search engines and social media
- **Media Galleries** - Display your images and videos with captions in a customizable gallery
- **Mobile Friendly** - Look amazing on every screen with a mobile friendly version of your site
- **Multi-language** - 15+ language packs including English, 中文, and Português
- **Multi-user** - Each author gets their own profile page
- **Privacy Pack** - Assists with GDPR
- **Stand Out** - Bring your site to life with animation, parallax backgrounds, and scroll effects
- **One-Click Deployment** - No servers. No databases. Only files.

## Themes

Academic comes with **automatic day (light) and night (dark) mode** built-in. Alternatively, visitors can  choose their preferred mode - click the sun/moon icon in the top right of the [Demo](https://academic-demo.netlify.com/) to see it in action! Day/night mode can also be disabled by the site admin in `params.toml`.

[Choose a stunning **theme** and **font**](https://sourcethemes.com/academic/themes/) for your site. Themes are fully [customizable](https://sourcethemes.com/academic/docs/customization/#custom-theme).

## Ecosystem

* **[Academic Admin](https://github.com/sourcethemes/academic-admin):** An admin tool to import publications from BibTeX or import assets for an offline site
* **[Academic Scripts](https://github.com/sourcethemes/academic-scripts):** Scripts to help migrate content to new versions of Academic

## Install

You can choose from one of the following four methods to install:

* [**one-click install using your web browser (recommended)**](https://sourcethemes.com/academic/docs/install/#install-with-web-browser)
* [install on your computer using **Git** with the Command Prompt/Terminal app](https://sourcethemes.com/academic/docs/install/#install-with-git)
* [install on your computer by downloading the **ZIP files**](https://sourcethemes.com/academic/docs/install/#install-with-zip)
* [install on your computer with **RStudio**](https://sourcethemes.com/academic/docs/install/#install-with-rstudio)

Then [personalize and deploy your new site](https://sourcethemes.com/academic/docs/get-started/).

## Updating

[View the Update Guide](https://sourcethemes.com/academic/docs/update/).

Feel free to *star* the project on [Github](https://github.com/gcushen/hugo-academic/) to help keep track of [updates](https://sourcethemes.com/academic/updates).

## License

Copyright 2016-present [George Cushen](https://georgecushen.com).

Released under the [MIT](https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md) license.
 --&gt;&lt;blockquote&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hive-JDBC Storage Handler</title>
      <link>/post/hive-jdbc/</link>
      <pubDate>Tue, 28 Jul 2015 00:00:00 +0000</pubDate>
      <guid>/post/hive-jdbc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
